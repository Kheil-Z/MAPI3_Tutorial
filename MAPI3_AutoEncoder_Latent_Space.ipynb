{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMiKQ0dVnHXVtuxsy3qumYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kheil-Z/MAPI3_Tutorial/blob/main/MAPI3_AutoEncoder_Latent_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning and AutoEncoders.\n",
        "The goal of this notebook is to understand and handle AutoEncoders using Pytorch.\n",
        "\n",
        "It is recommended to run the notebook on Colab over a GPU device ([how to use a GPU on colab ](https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html)).\n",
        "\n",
        "Author: K. Ziad\n",
        "\n",
        "Inspired by : https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial1.html\n",
        "\n",
        "## Guidelines :\n",
        "Carefuly read the given code then\n",
        "*   complete cells marked with a TODO.\n",
        "*   answer questions marked with a QUESTION .\n",
        "\n"
      ],
      "metadata": {
        "id": "jBO9MRFeqdZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55YZK3m5qXr3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "\n",
        "# Set numpy seed\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set torch seeds\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set device:\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on {device}\")"
      ],
      "metadata": {
        "id": "X-Ss9tfIjb5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) The dataset"
      ],
      "metadata": {
        "id": "LlAqYqzOmM5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to apply our AutoEncoder to the MNIST dataset, we can start by downloading it from Pytorch."
      ],
      "metadata": {
        "id": "syytSAHgZaX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Downloading dataset and creating Torch Dataloaders:\n",
        "\n",
        "# Download the MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root = \"./data\",\n",
        "                         train = True,\n",
        "                         download = True,\n",
        "                         transform = transforms.ToTensor())\n",
        "val_dataset = datasets.MNIST(root = \"./data\",\n",
        "                         train = False,\n",
        "                         download = True,\n",
        "                         transform = transforms.ToTensor())"
      ],
      "metadata": {
        "id": "i_qtt3ptrCrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following list will be useful for plotting specific digits (see 3) Generative Models)."
      ],
      "metadata": {
        "id": "9gM0FZv6Ey-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_digits_idx = []\n",
        "for digit in range(10):\n",
        "  idx_image = 0\n",
        "  while val_dataset[idx_image][1] != digit :\n",
        "    idx_image+=1\n",
        "  all_digits_idx.append(idx_image)\n",
        "\n",
        "fig,axs = plt.subplots(1,10, sharey=True, figsize=(16,8))\n",
        "for i,idx in enumerate(all_digits_idx):\n",
        "  axs[i].imshow(val_dataset[idx][0].view(28,28))\n",
        "  axs[i].set_title(f\"Label: {val_dataset[idx][1]}\")"
      ],
      "metadata": {
        "id": "waoa2maKEyBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that for the rest of this notebook, we will be exploring the realm of Unsupervized Learning approaches. Althugh MNIST is a dataset comrpised of images as well as lassociated labels, we will intentionally disregard the labels, and focus solely on the pixel information. \n",
        "\n",
        "With this in mind, the goal of the notebook is to explore how we can work with only pixel values. Several possibilities arise, we focus on AutoEncoders and the concept of *Latent Spaces*. \n",
        "\n",
        "Without loss of generality, an AutoEncoder is comprised of **encoder**, a **bottleneck** and a **decoder** :\n",
        "*  The encoder maps data from a high-dimensional input to the bottleneck where the sub-space is the smallest.\n",
        "* The decoder converst the encoded input back to the original input space.\n",
        "* The space of the bottleneck is what we commonly refer to as the **latent space**. It is usually much smaller than the input space, but more informative. It can be seen as a compressed version of the image space in our case. Usually AutoEncoders look something like this:\n",
        "\n",
        "![AE.webp](data:image/webp;base64,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)\n",
        "\n",
        "\n",
        "(Image from [this article](https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df))\n",
        "\n",
        "Thoughouth the notebook, ty to have a look at this figure and identify the different components of our networks.\n"
      ],
      "metadata": {
        "id": "p-rLq1JBi8lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1) Helper Functions"
      ],
      "metadata": {
        "id": "cqkpNl1KmWWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take time to understand and complete the following helper functions. What do they do?"
      ],
      "metadata": {
        "id": "pkY4JgOG3QnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(autoencoder, train_dataset, val_dataset, batch_size, device, epochs=20, lr=1e-3, weight_decay=1e-5):\n",
        "  \"\"\"\n",
        "  Function to train a given autoencoder\n",
        "\n",
        "  Input:\n",
        "    autoencoder: a torch Autoencoder \n",
        "    train_dataset: Torch dataset containing training samples.\n",
        "    val_dataset: Torch dataset containing validation samples.\n",
        "    batch_size: Batch size to construct dataloaders.\n",
        "    device: Device to run model on.\n",
        "    epochs: Number of epochs [default: 20]\n",
        "    lr: Learning rate for ADAM optimizer [default: 1e-3]\n",
        "    weight_decay: Weight decay factor for ADAM optimizer [default: 1e-5]\n",
        "\n",
        "  Returns:\n",
        "    loss_train: List of losses over each epoch during training phase.\n",
        "    loss_val: List of losses over each epoch during validation phase.\n",
        "  \"\"\"\n",
        "  # Set model to device, construct dataloaders and optimizer\n",
        "  autoencoder.to(device)\n",
        "  optimizer = torch.optim.Adam(autoencoder.parameters(),\n",
        "                           lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "  criterion = nn.MSELoss()\n",
        "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                     batch_size = batch_size,\n",
        "                                     shuffle = True)\n",
        "  val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                     batch_size = batch_size,\n",
        "                                     shuffle = False)\n",
        "  # Initialize empty loss lists\n",
        "  loss_train, loss_val = [],[]\n",
        "\n",
        "  # Run training loop\n",
        "  for epoch in tqdm(range(epochs), desc='Epoch'):\n",
        "    ####### Train loop #######\n",
        "    autoencoder.train()\n",
        "    running_loss = 0.0\n",
        "    for image, _ in train_loader:\n",
        "      # ===================forward=====================\n",
        "      image = image.to(device)\n",
        "      reconstructed_image = autoencoder(image)\n",
        "      loss = criterion(reconstructed_image.view(batch_size, -1), image.view(batch_size, -1))\n",
        "      # ===================backward====================\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Log the batch loss\n",
        "      running_loss += loss.detach() * image.size(0)\n",
        "    # Append the epoch's loss:\n",
        "    loss_train.append(running_loss.cpu()/len(train_dataset))\n",
        "\n",
        "    ####### Val loop #######\n",
        "    autoencoder.eval()\n",
        "    running_loss = 0.0\n",
        "    for image, _ in val_loader:\n",
        "      ### TODO: Complete the validation loop of the function ####\n",
        "      # ===================forward=====================\n",
        "      ...\n",
        "      # Log the batch loss\n",
        "      ...\n",
        "    # Append the epoch's loss:\n",
        "    ...\n",
        "\n",
        "  autoencoder.to('cpu')\n",
        "  return loss_train, loss_val"
      ],
      "metadata": {
        "id": "gv9SFnvbmY0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_samples(autoencoder, dataset, num_to_plot=10):\n",
        "  fig, axs = plt.subplots(2,10, figsize=(80,10))\n",
        "  for i in range(num_to_plot):\n",
        "    sample = dataset[np.random.randint(len(dataset))][0].view(-1,1,28,28)\n",
        "    reconstruction = autoencoder(sample)\n",
        "\n",
        "    axs[0][i].imshow(sample.view(28, 28))\n",
        "    axs[1][i].imshow(reconstruction.detach().view(28, 28))\n",
        "  plt.subplots_adjust(wspace=0, hspace=0)\n",
        "  plt.show()\n",
        "  return None"
      ],
      "metadata": {
        "id": "7M9oJSQt1C9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Setting Hyper-parameters"
      ],
      "metadata": {
        "id": "Rs7uc9VV8dQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters for all of our models. You may change this at will after running the notebook\n",
        "\n",
        "latent_dim = 20 # Size of our latent dimension\n",
        "batch_size = 64 \n",
        "num_epochs = 15\n",
        "learning_rate = 1e-3\n",
        "weight_decay_value = 1e-5"
      ],
      "metadata": {
        "id": "G66-AfNU8h0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) The Models"
      ],
      "metadata": {
        "id": "ecQI8sj2maMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can begin by simply implementing a Linear AutoEncoder. It is composed of two fully connected layers, one for encoding our input and one for decoding our latent space.\n",
        "\n",
        " In order to ensure strict linearity for now, no activation functions are used."
      ],
      "metadata": {
        "id": "FTk5x7n48wmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Linear AutoEncoder"
      ],
      "metadata": {
        "id": "FGwX1e1XkP_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAutoEncoder(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(LinearAutoEncoder, self).__init__()\n",
        "\n",
        "    # Encoder layer (a linear mapping from input size to hidden_dimension)\n",
        "    self.linear_encoder = nn.Linear(28 * 28, latent_dim)\n",
        "\n",
        "    # Decoder layer (a linear mapping from hidden_dimension to input size)\n",
        "    self.linear_decoder = nn.Linear(latent_dim, 28 * 28)\n",
        "\n",
        "  def encode(self, x):\n",
        "    # Encode the input sample\n",
        "    latent = self.linear_encoder(x.view(x.size(0), -1))\n",
        "    return latent\n",
        "\n",
        "  def decode(self, latent):\n",
        "    # Decode the latent vector\n",
        "    x_prime = self.linear_decoder(latent).view(-1,1,28,28)\n",
        "    return x_prime\n",
        "\n",
        "  def forward(self, x):\n",
        "    latent = self.encode(x)\n",
        "    return self.decode(latent)"
      ],
      "metadata": {
        "id": "_Yn6_eHukNCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct AE\n",
        "linear_ae = LinearAutoEncoder(latent_dim)\n",
        "\n",
        "summary(linear_ae, (1, 28, 28))"
      ],
      "metadata": {
        "id": "Dqt_2jjR8Ka4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Train model:\n",
        "\n",
        "### TODO:  Train the AE using the previously defined hyper-parameters ###\n",
        "linear_ae_loss_train, linear_ae_loss_val = ...\n",
        "\n",
        "### TODO: Plot the Reconstruction train and validation losses of the Linear AE ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "wDLdmJDbkNFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some reconstructions:\n",
        "plot_samples(linear_ae, val_dataset, num_to_plot=10)"
      ],
      "metadata": {
        "id": "SV9e2UMr2JR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QUESTION\n",
        " What do you think of these sample reconstructions? What easy steps could you implement in order to obtain more accurate reconstructions?"
      ],
      "metadata": {
        "id": "o7U_m90B4mCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:  ???"
      ],
      "metadata": {
        "id": "I-7gNQTr4114"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### QUESTION\n",
        " In terms of dimensionality-reduction abilities, how do you think such a model compares to other algorithms you may have seen such as PCA?"
      ],
      "metadata": {
        "id": "A2graYdf9TtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: ???"
      ],
      "metadata": {
        "id": "Roa9PYJv9eT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Convolutional AutoEncoder (Non Linear)"
      ],
      "metadata": {
        "id": "81ano_1yvxGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to implement a Convolutional AutoEncoder, we will be using Transpose Convolutions in order to upsample our latent space.\n",
        "\n",
        "#### QUESTION\n",
        "\n",
        " Using [Pytorch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) and [this explanation](https://d2l.ai/chapter_computer-vision/transposed-conv.html), explain in your own words how a Transpose convolution works. Does it correspond to a \"deconvolution\" in the sens of the inverse of the convolution operator?"
      ],
      "metadata": {
        "id": "FIlhPp56Fgzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: ???"
      ],
      "metadata": {
        "id": "DvRMSJbbHT0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is an example implementation of a Convolutional AE using pytorch. It takes an MNIST image (1,28,28) image, convolves it to a (64,1,1) feature map\n",
        "\n",
        "*   It takes an MNIST image (1,28,28) image,\n",
        "*   Convolves it to a (64,1,1) feature map,\n",
        "*   Flattens the feature map, into a vector of shape 64,\n",
        "*   Reduces this to the given latent_dimension size,\n",
        "*   Then decodes the latent vector back to an image.\n",
        "\n",
        "Complete the class implementation in order to have an AE which functions as aforementioned. (You may want to use [Pytroch's documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for a refresher on convolutional arithmetic).\n"
      ],
      "metadata": {
        "id": "gjikkyl15RrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoEncoder(nn.Module):\n",
        "    def __init__(self,latent_dim):\n",
        "        super(ConvAutoEncoder, self).__init__()\n",
        "        # Input (batch_size, 1, 28, 28)\n",
        "        self.encoder = nn.Sequential(\n",
        "            ### TODO: Complete the missing values ###\n",
        "            nn.Conv2d(..., 16, 3, stride=2, padding=1), # (batch_size, 16, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(..., ..., 3, stride=2, padding=1), # (batch_size, 32, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, ...), # (batch_size, 64, 1, 1)\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "            )\n",
        "        \n",
        "        ### TODO: Complete the missing values ###\n",
        "        self.fc_encoder = nn.Linear(..., latent_dim)\n",
        "        self.fc_decoder = nn.Linear(..., 64)\n",
        "        \n",
        "        \n",
        "        #64, 1, 1\n",
        "        self.decoder = nn.Sequential(\n",
        "            ### TODO: Complete the missing values ###\n",
        "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
        "            nn.ConvTranspose2d(64, ..., 7), # (batch_size, 32, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # (batch_size, 16, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(..., ..., 3, stride=2, padding=1, output_padding=1), # (batch_size,1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        \n",
        "    def encode(self, x):\n",
        "        ### TODO: Complete the encode method ###\n",
        "        x = ....\n",
        "        latent = ....\n",
        "        return latent\n",
        "\n",
        "    def decode(self, latent):\n",
        "      ### TODO: Complete the decode method ###\n",
        "        s = F.relu(...)\n",
        "        x_prime = ...\n",
        "        return x_prime\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encode(x)\n",
        "        x_prime = self.decode(latent)\n",
        "        return x_prime"
      ],
      "metadata": {
        "id": "D0AEL2LlkNH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QUESTION\n",
        " Why is the final activation of our network a Sigmoid? What other activation functions could we have used?"
      ],
      "metadata": {
        "id": "wqJkryECKQvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : ???"
      ],
      "metadata": {
        "id": "gwy84QlqKakS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct AE\n",
        "conv_ae = ConvAutoEncoder(latent_dim)\n",
        "\n",
        "summary(conv_ae, (1, 28, 28))"
      ],
      "metadata": {
        "id": "llHXTCFz9lKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Train model:\n",
        "\n",
        "### TODO:  Train the AE using the previously defined hyper-parameters ###\n",
        "conv_ae_loss_train, conv_ae_loss_val = ....\n",
        "\n",
        "### TODO: Plot the Reconstruction train and validation losses of the Convolutional AE ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "### TODO: Plot the Reconstruction losses of both models ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zOO0-32DkNKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QUESTION\n",
        " Which of both models performs better? Why do you think that is?"
      ],
      "metadata": {
        "id": "xCBUUWHTJSHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : ???"
      ],
      "metadata": {
        "id": "pRo19P4NJfH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some reconstructions:\n",
        "plot_samples(conv_ae, val_dataset, num_to_plot=10)"
      ],
      "metadata": {
        "id": "DiGyD96Q2CMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Variational AutoEncoder (VAEs)"
      ],
      "metadata": {
        "id": "rKJ_vzjE0oNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        # Input: (batch_size, 1, 28, 28)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1), #(batch_size, 16, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1), #(batch_size, 32, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 7), #(batch_size, 64, 1, 1)\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "            )\n",
        "        \n",
        "        \n",
        "        self.fc_mu = nn.Linear(64, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
        "        \n",
        "        self.fc_decoder = nn.Linear(latent_dim, 64)\n",
        "        \n",
        "        \n",
        "        # Input: (batch_size, 64, 1, 1)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(dim=-1, unflattened_size=(64,1,1)),\n",
        "            nn.ConvTranspose2d(64, 32, 7), #(batch_size, 32, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), #(batch_size, 16, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), #(batch_size, 1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        z = mu + (std * epsilon)\n",
        "        return z\n",
        "\n",
        "    \n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Get mu and logvar or reparam trick\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "\n",
        "        latent = self.reparameterize(mu, logvar)\n",
        "        return latent, mu, logvar\n",
        "\n",
        "    def decode(self, latent):\n",
        "        s = F.relu(self.fc_decoder(latent))\n",
        "        x_prime = self.decoder(s)\n",
        "        return x_prime\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent, mu, logvar = self.encode(x)\n",
        "       \n",
        "        x_prime = self.decode(latent)\n",
        "        return x_prime, mu, logvar"
      ],
      "metadata": {
        "id": "AjOq0GRu_WZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a re-implementation of the train function for the VAE."
      ],
      "metadata": {
        "id": "Gpqb67P5IVjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def VAE_loss_fn(mse_loss, mu, logvar, beta=1):\n",
        "    KL_Div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return mse_loss + beta*KL_Div\n",
        "\n",
        "\n",
        "\n",
        "def train_VAE(vae, train_dataset, val_dataset, batch_size, device, epochs=20, lr=1e-3, weight_decay=1e-5, beta=1):\n",
        "  \"\"\"\n",
        "    Function to train a given autoencoder\n",
        "\n",
        "    Input:\n",
        "      vae: a VAE model \n",
        "      train_dataset: Torch dataset containing training samples.\n",
        "      val_dataset: Torch dataset containing validation samples.\n",
        "      batch_size: Batch size to construct dataloaders.\n",
        "      device: Device to run model on.\n",
        "      epochs: Number of epochs [default: 20]\n",
        "      lr: Learning rate for ADAM optimizer [default: 1e-3]\n",
        "      weight_decay: Weight decay factor for ADAM optimizer [default: 1e-5]\n",
        "      beta: weight of Kullback-Leibler divergence in VAE loss function\n",
        "\n",
        "    Returns:\n",
        "      loss_train: List of losses over each epoch during training phase.\n",
        "      loss_val: List of losses over each epoch during validation phase.\n",
        "      mse_loss_train\n",
        "      mse_loss_val\n",
        "  \"\"\"\n",
        "  # Set model to device, construct dataloaders and optimizer\n",
        "  vae.to(device)\n",
        "  optimizer = torch.optim.Adam(vae.parameters(),\n",
        "                           lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "  MSE_criterion = nn.MSELoss(reduction='sum')\n",
        "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                     batch_size = batch_size,\n",
        "                                     shuffle = True)\n",
        "  val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                     batch_size = batch_size,\n",
        "                                     shuffle = False)\n",
        "  # Initialize empty loss lists\n",
        "  loss_train, loss_val = [],[]\n",
        "  mse_loss_train, mse_loss_val = [],[]\n",
        "  # Run training loop\n",
        "  for epoch in tqdm(range(epochs), desc='Epoch'):\n",
        "    ####### Train loop #######\n",
        "    vae.train()\n",
        "    running_loss, running_loss_mse = 0.0, 0.0\n",
        "    for image, _ in train_loader:\n",
        "        # ===================forward=====================\n",
        "      image = image.to(device)\n",
        "      reconstruction, mu, logvar  = vae(image)\n",
        "      # Compute loss elements :\n",
        "      mse_loss = MSE_criterion(reconstruction.view(batch_size, -1), image.view(batch_size, -1))\n",
        "      loss = VAE_loss_fn(mse_loss, mu, logvar, beta=beta)\n",
        "      # ===================backward====================\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Log the batch loss\n",
        "      running_loss += loss.detach() * image.size(0)\n",
        "      running_loss_mse += mse_loss.detach() * image.size(0)\n",
        "    # Append the epoch's loss:\n",
        "    loss_train.append(running_loss.cpu()/len(train_dataset))\n",
        "    mse_loss_train.append(running_loss_mse.cpu()/len(train_dataset))\n",
        "\n",
        "    ####### Val loop #######\n",
        "    vae.eval()\n",
        "    running_loss, running_loss_mse = 0.0, 0.0\n",
        "    for image, _ in val_loader:\n",
        "      # ===================forward=====================\n",
        "      image = image.to(device)\n",
        "      reconstruction, mu, logvar  = vae(image)\n",
        "      # Compute loss elements :\n",
        "      mse_loss = MSE_criterion(reconstruction.view(batch_size, -1), image.view(batch_size, -1))\n",
        "      loss = VAE_loss_fn(mse_loss, mu, logvar, beta=beta)\n",
        "\n",
        "      # Log the batch loss\n",
        "      running_loss += loss.detach() * image.size(0)\n",
        "      running_loss_mse += mse_loss.detach() * image.size(0)\n",
        "    # Append the epoch's loss:\n",
        "    loss_val.append(running_loss.cpu()/len(val_dataset))\n",
        "    mse_loss_val.append(running_loss_mse.cpu()/len(val_dataset))\n",
        "\n",
        "  vae.to('cpu')\n",
        "  return loss_train, loss_val, mse_loss_train, mse_loss_val"
      ],
      "metadata": {
        "id": "IWJiEr-AIT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct AE\n",
        "vae_model = VAE(latent_dim)\n",
        "\n",
        "summary(vae_model, (1, 28, 28))"
      ],
      "metadata": {
        "id": "aO-O-14l-7YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Train VAE\n",
        "\n",
        "### TODO:  Train the VAE using the previously defined hyper-parameters ###\n",
        "vae_loss_train, vae_loss_val, vae_loss_train_mse, vae_loss_val_mse = ...\n",
        "\n",
        "### TODO: Plot the Reconstruction train and validation losses of the Convolutional AE ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "### TODO: Plot the Reconstruction losses of all three models ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tm0cvkis3-hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### QUESTION\n",
        " Why does the plotted Loss seem to indicate that the VAE has a terrible reconstruction compared to the previous models? Is it the case?"
      ],
      "metadata": {
        "id": "hX5AG67iN49d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: ???"
      ],
      "metadata": {
        "id": "p1TEQi7nOIOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: Plot the VAE's training MSE and KLD loss seperately. Recall that VAE_loss = mse_loss + KL_Div (if you kept beta=1) ###\n",
        "plt.plot(...)\n",
        "plt.plot(...)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Ftn54vJaBfXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: Read the plot_samples helper function again, and complete the script in order to compare sample reconstructions from all three trained models  ###\n",
        "\n",
        "fig, axs = plt.subplots(4,10, figsize=(60,10))\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(10):\n",
        "  # Get a random sample from the MNIST validation set.\n",
        "  sample = ... # TODO\n",
        "\n",
        "  # Forward through trained models to get reconstructions.\n",
        "  reconstruction_linear_ae = ... # TODO\n",
        "  reconstruction_conv_ae = ... # TODO\n",
        "  reconstruction_vae, _, _ = ... # TODO\n",
        "\n",
        "  # Evaluate the reconstruction error.\n",
        "  lin_ae_mse = ... # TODO\n",
        "  conv_ae_mse = ... # TODO\n",
        "  vae_mse = ... # TODO\n",
        "\n",
        "  # Display everything\n",
        "  axs[0][i].imshow(sample.view(28, 28))\n",
        "  axs[1][i].imshow(reconstruction_linear_ae.detach().view(28, 28))\n",
        "  axs[1][i].set_title(str(lin_ae_mse))\n",
        "  axs[2][i].imshow(reconstruction_conv_ae.detach().view(28, 28))\n",
        "  axs[2][i].set_title(str(conv_ae_mse))\n",
        "  axs[3][i].imshow(reconstruction_vae.detach().view(28, 28))\n",
        "  axs[3][i].set_title(str(vae_mse))\n",
        "#plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b2m-KHhy09Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# As Generative Models:"
      ],
      "metadata": {
        "id": "tzp4hr7AHAmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoEncoders are not only useful for dimensionality-reduction. They also encapsulate generating capabilities, and sometimes display interesting properties in terms of their Latent Space.\n",
        "\n",
        "#### QUESTION\n",
        " What does the following function do?"
      ],
      "metadata": {
        "id": "zxagD9mEAkdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : ???"
      ],
      "metadata": {
        "id": "mPmVv097A_wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(ae, x_1, x_2, n=12):\n",
        "    if not isinstance(ae,VAE):\n",
        "      z_1 = ae.encode(x_1.view(-1,1,28,28))\n",
        "      z_2 = ae.encode(x_2.view(-1,1,28,28))\n",
        "    else:\n",
        "      z_1, mu_1, logvar_1 = ae.encode(x_1.view(-1,1,28,28))\n",
        "      z_2, mu_2, logvar_2 = ae.encode(x_2.view(-1,1,28,28))\n",
        "    image = np.zeros((28,28*n))\n",
        "    for i, t in enumerate(np.linspace(0, 1, n)):\n",
        "      if not isinstance(ae,VAE):\n",
        "        recons = ae.decode(z_1 + (z_2 - z_1)*t).view(28,28).detach().numpy()\n",
        "      else:\n",
        "        mu = mu_1 + (mu_2 - mu_1)*t\n",
        "        logvar = logvar_1 + (logvar_2 - logvar_1)*t\n",
        "        latent = ae.reparameterize(mu, logvar)\n",
        "        recons = ae.decode(latent).detach().numpy()\n",
        "      image[:, 28*i:28*(i+1)] = recons\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xiT0RMh_kNWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Try to subsample different digits:\n",
        "idx_1, idx_2 = all_digits_idx[4], all_digits_idx[6]\n",
        "img_1, img_2 = val_dataset[idx_1][0], val_dataset[idx_2][0]\n",
        "\n",
        "# uncoment for random:\n",
        "# img_1, img_2 = val_dataset[np.random.randint(len(val_dataset))][0], val_dataset[np.random.randint(len(val_dataset))][0]\n",
        "\n",
        "interpolate(linear_ae, img_1, img_2)\n",
        "interpolate(conv_ae, img_1, img_2)\n",
        "interpolate(vae_model, img_1, img_2)"
      ],
      "metadata": {
        "id": "Scun7ziu4N5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QUESTION\n",
        " What can you say abouth the latent spaces of these models?"
      ],
      "metadata": {
        "id": "6hnCK_7vBYRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : ??? "
      ],
      "metadata": {
        "id": "3hEss-yaBfFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### QUESTION\n",
        " What does the following function do?"
      ],
      "metadata": {
        "id": "grfP5UjaCBJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO :  ???"
      ],
      "metadata": {
        "id": "I3yfUX8WCElI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(ae, n=10):\n",
        "  images = np.zeros((28,28*n))\n",
        "  for i in range(n):\n",
        "    if not isinstance(ae,VAE):\n",
        "      latent = torch.rand(1,latent_dim)\n",
        "      image = ae.decode(latent).view(1, 28,28).detach().numpy()\n",
        "    else:\n",
        "      mu = torch.zeros(1, latent_dim)\n",
        "      logvar = torch.ones(1, latent_dim)\n",
        "      latent = ae.reparameterize(mu, logvar)\n",
        "      image = ae.decode(latent).view(1, 28,28).detach().numpy()\n",
        "    images[:, 28*i:28*(i+1)] = image[0]\n",
        "  plt.figure(figsize=(15,10))\n",
        "  plt.imshow(images)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "generate(linear_ae)\n",
        "generate(conv_ae)\n",
        "generate(vae_model)"
      ],
      "metadata": {
        "id": "-JbE_c79kNZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : ??? "
      ],
      "metadata": {
        "id": "Y_ZLW13Vb7-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AhP9cNnDN2S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus:"
      ],
      "metadata": {
        "id": "LmbOGX39ELDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the [Scikit-Learn's documentation on T-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), further inspect the latent dimension of our three models.\n",
        "\n",
        "(If you want to read more on T-SNE and latent spaces : \n",
        "* [nice notebook](https://notebook.community/nikbearbrown/Deep_Learning/NEU/Nik_Bear_Brown_DL/Latent_Space_Visualization/NBB_Latent_Space_Visualization)\n",
        "* [Video on Latent spaces](https://www.youtube.com/watch?v=7Pcvdo4EJeo)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ILtENgNIEPOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : using T-SNE embedding, visualize the latent space of all three models sparetely on a 2-D plot.\n"
      ],
      "metadata": {
        "id": "iDNljvZtDo6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IY0RISVYcu1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}